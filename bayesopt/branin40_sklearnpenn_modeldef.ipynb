{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "parameters"
    ]
   },
   "outputs": [],
   "source": [
    "gpu=0\n",
    "re_calib=False\n",
    "re_bias_y=False\n",
    "dataset='ackley'\n",
    "log_root='int_log'\n",
    "model='small'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "\n",
    "from argparse import Namespace\n",
    "import copy\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.utils._testing import ignore_warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "\n",
    "from ackley import Ackley\n",
    "from probo import NelderMeadAcqOptimizer, SimpleBo\n",
    "from probo.util.misc_util import dict_to_namespace\n",
    "from branin import branin, get_branin_domain_nd\n",
    "from penn_sklearn import SklearnPenn\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from models import *\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Args:\n",
    "    def __init__(self):\n",
    "        self.log_root=log_root\n",
    "        self.incremental = True # If true we don't retrain the model every step\n",
    "        self.dataset = dataset  # This is branin or ackley\n",
    "        \n",
    "        self.re_calib = re_calib\n",
    "        self.re_bias_f = False\n",
    "        self.re_bias_y = re_bias_y\n",
    "\n",
    "        # Modeling parameters\n",
    "        self.model = model\n",
    "        self.learning_rate = 1e-3\n",
    "        self.num_bins = 0\n",
    "        self.knn = 10\n",
    "\n",
    "        # Run related parameters\n",
    "        self.gpu = gpu\n",
    "        self.num_iter = 1000\n",
    "        self.run_label = 0\n",
    "        self.num_run = 10\n",
    "        self.flow_skip = 1\n",
    "        \n",
    "args = Args()\n",
    "\n",
    "device = torch.device('cuda:%d' % args.gpu)\n",
    "args.device = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "if args.num_bins == 0:\n",
    "    eval_bias = eval_bias_knn\n",
    "    assert args.knn >= 10 and args.knn % 2 == 0\n",
    "    \n",
    "\n",
    "while True:\n",
    "    args.name = '%s/model=%s-%r-%r-%r-%r-bin=%d-%d-run=%d' % \\\n",
    "        (args.dataset, args.model, args.incremental, args.re_calib, args.re_bias_f, args.re_bias_y, args.num_bins, args.knn, args.run_label)\n",
    "    args.log_dir = os.path.join(args.log_root, args.name)\n",
    "    if not os.path.isdir(args.log_dir):\n",
    "        os.makedirs(args.log_dir)\n",
    "        break\n",
    "    args.run_label += 1\n",
    "print(\"Run number = %d\" % args.run_label)\n",
    "writer = SummaryWriter(args.log_dir)\n",
    "log_writer = open(os.path.join(args.log_dir, 'results.txt'), 'w')\n",
    "\n",
    "global_iteration = 0\n",
    "random.seed(args.run_label)  # Set a different random seed for different run labels\n",
    "torch.manual_seed(args.run_label)\n",
    "\n",
    "def log_scalar(name, value, epoch):\n",
    "    writer.add_scalar(name, value, epoch)\n",
    "    log_writer.write('%f ' % value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define probabilistic ensemble of neural networks (PENN) model\n",
    "\n",
    "class TorchPenn:\n",
    "    \"\"\"\n",
    "    Probabilistic ensemble neural network (PENN) model implemented in\n",
    "    scikit-learn.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, params=None, verbose=True):\n",
    "        \"\"\"\n",
    "        Parameters\n",
    "        ----------\n",
    "        params : Namespace_or_dict\n",
    "            Namespace or dict of parameters for this model.\n",
    "        verbose : bool\n",
    "            If True, print description string.\n",
    "        \"\"\"\n",
    "        self.set_params(params)\n",
    "        if verbose:\n",
    "            print(f'*[INFO] Model=SklearnPenn with params={self.params}')\n",
    "        self.ensemble = []\n",
    "        self.optims = []\n",
    "        \n",
    "    def set_params(self, params):\n",
    "        \"\"\"Set self.params, the parameters for this model.\"\"\"\n",
    "        params = dict_to_namespace(params)\n",
    "\n",
    "        self.params = Namespace()\n",
    "        self.params.n_ensemble = getattr(params, 'n_ensemble', 5)\n",
    "        self.params.hls = getattr(params, 'hls', (20, 30, 40))\n",
    "        self.params.max_iter = getattr(params, 'max_iter', 500)\n",
    "        self.params.alpha = getattr(params, 'alpha', 0.01)\n",
    "        self.params.trans_x = getattr(params, 'trans_x', False)\n",
    "\n",
    "    def set_data(self, data):\n",
    "        \"\"\"Set self.data.\"\"\"\n",
    "        self.data = copy.deepcopy(data)\n",
    "\n",
    "    @ignore_warnings(category=ConvergenceWarning)\n",
    "    def inf(self, data):\n",
    "        global global_iteration\n",
    "        \"\"\"Set data, run inference.\"\"\"\n",
    "        self.set_data(data)\n",
    "        x_train = torch.from_numpy(np.array(self.data.x)).type(torch.float32).to(args.device)\n",
    "        y_train = torch.from_numpy(np.array(self.data.y)).view(-1, 1).type(torch.float32).to(args.device)\n",
    "        print(x_train.shape, y_train.shape)\n",
    "        \n",
    "        if not args.incremental or len(self.ensemble) == 0:\n",
    "            self.ensemble = [model_list[args.model](x_train.shape[1]).to(device) for i in range(self.params.n_ensemble)]\n",
    "            self.optims = [optim.Adam(self.ensemble[i].parameters(), lr=args.learning_rate) for i in range(self.params.n_ensemble)]\n",
    "            if args.re_calib or args.re_bias_f or args.re_bias_y:\n",
    "                for model in self.ensemble:\n",
    "                    model.recalibrator = RecalibratorOnline(model, args, re_calib=args.re_calib, re_bias_f=args.re_bias_f, re_bias_y=args.re_bias_y)\n",
    "        for model, optimizer in zip(self.ensemble, self.optims):\n",
    "            # Define model and optimizer\n",
    "            # scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=args.num_epoch // 20, gamma=0.9) \n",
    "            # train_bb_iter = itertools.cycle(train_bb_loader)\n",
    "        \n",
    "            if args.re_bias_f or args.re_bias_y or args.re_calib:\n",
    "                randperm = torch.randperm(x_train.shape[0])\n",
    "                x_train, y_train = x_train[randperm], y_train[randperm]\n",
    "                num_train = x_train.shape[0] * 2 // 3\n",
    "                x_val, y_val = x_train[num_train:], y_train[num_train:]\n",
    "                x_train, y_train = x_train[:num_train], y_train[:num_train]\n",
    "            \n",
    "            model.train()\n",
    "            # print(args.num_epoch)\n",
    "            for epoch in range(args.num_iter):\n",
    "                # Minimize L2\n",
    "                optimizer.zero_grad()\n",
    "                loss_l2 = eval_l2(model, (x_train, y_train), args)\n",
    "                loss_l2.mean().backward()\n",
    "                optimizer.step()\n",
    "                # scheduler.step()\n",
    "                # writer.add_scalar('loss_l2', loss_l2.mean(), global_iteration)\n",
    "                \n",
    "                if model.recalibrator is not None:\n",
    "                    model.recalibrator.train_step((x_val, y_val))\n",
    "                global_iteration += 1\n",
    "                # if epoch % 499 == 0:\n",
    "                #    print(\"Time elapsed = %.3f, loss=%.3f\" % (time.time() - start_time, loss_l2.mean().cpu().item())) \n",
    "            model.eval()\n",
    "            \n",
    "\n",
    "\n",
    "    def post(self, s):\n",
    "        \"\"\"Return one posterior sample\"\"\"\n",
    "        return np.random.choice(self.ensemble)\n",
    "\n",
    "    def gen_list(self, x_list, z, s, nsamp):\n",
    "        \"\"\"\n",
    "        Draw nsamp samples from generative process, given list of inputs x_list,\n",
    "        and posterior sample z.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        x_list : list\n",
    "            List of numpy ndarrays each with shape=(-1,).\n",
    "        z : Namespace\n",
    "            Namespace of GP hyperparameters.\n",
    "        s : int\n",
    "            [FOR THIS MODEL, YOU CAN IGNORE THIS.] The seed, a positive integer.\n",
    "        nsamp : int\n",
    "            The number of samples to draw from generative process.\n",
    "\n",
    "        Returns\n",
    "        -------\n",
    "        list\n",
    "            A list with len=len(x_list) of numpy ndarrays, each with shape=(nsamp,).\n",
    "        \"\"\"\n",
    "        # print(len(x_list), nsamp)\n",
    "        with torch.no_grad():\n",
    "            pred_list = [\n",
    "                z(torch.from_numpy(x.reshape(1, -1)).type(torch.float32).to(device).repeat(nsamp, 1)).cpu().numpy().reshape(-1)\n",
    "                for x in x_list\n",
    "            ]\n",
    "        return pred_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# BO Setup\n",
    "\n",
    "# define function\n",
    "\n",
    "if args.dataset == 'branin':\n",
    "    n_dim = 40\n",
    "    f = lambda x: np.sum([branin(x[2 * i : 2 * i + 2]) for i in range(n_dim // 2)])\n",
    "\n",
    "    # define model\n",
    "    model = TorchPenn()\n",
    "\n",
    "    # define acqfunction\n",
    "    acqfunction = {'acq_str': 'ts', 'n_gen': 500}\n",
    "\n",
    "    # define acqoptimizer\n",
    "    domain = get_branin_domain_nd(n_dim)\n",
    "    acqoptimizer = NelderMeadAcqOptimizer({'rand_every': 10, 'max_iter': 200, 'jitter': True}, domain)\n",
    "else:\n",
    "    assert args.dataset == 'ackley'\n",
    "    \n",
    "    # define function\n",
    "    n_dim = 10\n",
    "    f = Ackley(n_dim)\n",
    "\n",
    "    # define model\n",
    "    model = SklearnPenn()\n",
    "\n",
    "    # define acqfunction\n",
    "    acqfunction = {'acq_str': 'ts', 'n_gen': 500}\n",
    "\n",
    "    # define acqoptimizer\n",
    "    domain = f.get_domain()\n",
    "    acqoptimizer = NelderMeadAcqOptimizer({'rand_every': 10, 'max_iter': 200, 'jitter': True}, domain)\n",
    "\n",
    "# define initial dataset\n",
    "n_init = 100\n",
    "data = Namespace()\n",
    "data.x = domain.unif_rand_sample(n_init)\n",
    "data.y = [f(x) for x in data.x]\n",
    "\n",
    "# define BO\n",
    "n_iter = 200\n",
    "bo = SimpleBo(f, model, acqfunction, acqoptimizer, data=data, params={'n_iter': n_iter}, seed=args.run_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run BO\n",
    "\n",
    "results = bo.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "\n",
    "save_dir = Path(f'results/branin{n_dim}_sklp_{args.run_label}')\n",
    "save_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "with open(save_dir / \"results.pkl\", \"wb\") as handle:\n",
    "    pickle.dump(results, handle, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys = vars(vars(results)['data'])['y']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for y in ys:\n",
    "    log_writer.write('%f ' % y)\n",
    "log_writer.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_y = [ys[0]]\n",
    "for y in ys[1:]:\n",
    "    if y < min_y[-1]:\n",
    "        min_y.append(y)\n",
    "    else:\n",
    "        min_y.append(min_y[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(min_y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
